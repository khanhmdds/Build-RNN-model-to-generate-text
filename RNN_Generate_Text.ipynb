{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_Generate_Text.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU520VTUUSBA"
      },
      "source": [
        "## 1. Tải dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLE33nJlAksi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4938ee6c-b829-463c-ea05-79eb7c4b83fa"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/protonx-cloud-storage/data.txt\n",
        "data = open('data.txt').read()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-25 13:38:00--  https://storage.googleapis.com/protonx-cloud-storage/data.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.199.128, 74.125.20.128, 74.125.197.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.199.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 93578 (91K) [text/plain]\n",
            "Saving to: ‘data.txt’\n",
            "\n",
            "\rdata.txt              0%[                    ]       0  --.-KB/s               \rdata.txt            100%[===================>]  91.38K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2022-06-25 13:38:00 (88.3 MB/s) - ‘data.txt’ saved [93578/93578]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFKIhGwEUWib"
      },
      "source": [
        "## 2. Thêm các thư viện cần thiết"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SECXWx0krUlc"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow.keras.utils as ku \n",
        "import numpy as np "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SISI8yglUpL-"
      },
      "source": [
        "## 3. Xử lý dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3QHseGbAoXg"
      },
      "source": [
        "corpus = data.lower().split(\"\\n\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI96pGNyAr68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6dad144-fa94-45db-cdc3-0c5d793642d9"
      },
      "source": [
        "corpus[:4]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['from fairest creatures we desire increase,',\n",
              " \"that thereby beauty's rose might never die,\",\n",
              " 'but as the riper should by time decease,',\n",
              " 'his tender heir might bear his memory:']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPHJG_W9VJnL"
      },
      "source": [
        "### 3.1. Xây dựng tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej0_opwlVNeJ"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpfD1l3MVO9D"
      },
      "source": [
        "### 3.2. Tách câu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGlYY5-sVS24"
      },
      "source": [
        "Tách từng câu thành nhiều phần có chiều dài tăng dần để làm từng điểm dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKsfQdUaAwEA"
      },
      "source": [
        "input_sequences = []\n",
        "for line in corpus:\n",
        "  token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "  for i in range(1, len(token_list)):\n",
        "    n_gram_sequence = token_list[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_fKj2RiUx4c",
        "outputId": "486e6e3c-1317-4dc5-c865-e556432fcda8"
      },
      "source": [
        "input_sequences[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[34, 417],\n",
              " [34, 417, 877],\n",
              " [34, 417, 877, 166],\n",
              " [34, 417, 877, 166, 213],\n",
              " [34, 417, 877, 166, 213, 517],\n",
              " [8, 878],\n",
              " [8, 878, 134],\n",
              " [8, 878, 134, 351],\n",
              " [8, 878, 134, 351, 102],\n",
              " [8, 878, 134, 351, 102, 156]]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5njfzVusWFga",
        "outputId": "4350b375-06cd-44d9-cb47-56ebc4c980a6"
      },
      "source": [
        "tokenizer.sequences_to_texts([[34, 417]])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['from fairest']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToolcuzyVeBC",
        "outputId": "5191f3fb-bf7d-46f5-bbf7-e8232fb49a1f"
      },
      "source": [
        "for point in input_sequences[:10]:\n",
        "  print(\" \".join(tokenizer.sequences_to_texts([point])))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from fairest\n",
            "from fairest creatures\n",
            "from fairest creatures we\n",
            "from fairest creatures we desire\n",
            "from fairest creatures we desire increase\n",
            "that thereby\n",
            "that thereby beauty's\n",
            "that thereby beauty's rose\n",
            "that thereby beauty's rose might\n",
            "that thereby beauty's rose might never\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US982AN3DhzI"
      },
      "source": [
        "### 3.3. Chia features, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ely2LS9dWd4J"
      },
      "source": [
        "Thực hiện Padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ON9tySfWiM_"
      },
      "source": [
        "# pad sequences \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1nan5xGWm1f"
      },
      "source": [
        "Các câu được tách bên trên sẽ được padding với việc chèn các giá trị 0 vào trước câu để tạo ra các câu có chiều dài bằng nhau.\n",
        "\n",
        "Việc **padding vào trước** vì bài toán này ta sẽ sinh từ từ phía sau nên thông tin bên phải là những giá trị khác 0. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4EdCwU8Wjpt",
        "outputId": "b418f87e-a955-4c9f-c9c3-e1e5e627153e"
      },
      "source": [
        "input_sequences[:10]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,  34, 417],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  34, 417, 877],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  34, 417, 877, 166],\n",
              "       [  0,   0,   0,   0,   0,   0,  34, 417, 877, 166, 213],\n",
              "       [  0,   0,   0,   0,   0,  34, 417, 877, 166, 213, 517],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   8, 878],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   8, 878, 134],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   8, 878, 134, 351],\n",
              "       [  0,   0,   0,   0,   0,   0,   8, 878, 134, 351, 102],\n",
              "       [  0,   0,   0,   0,   0,   8, 878, 134, 351, 102, 156]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xSnhEr0XB1z"
      },
      "source": [
        "Cắt cột cuối cùng của ma trận bên trên để làm nhãn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PBrJTe4Dnsb"
      },
      "source": [
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "# Chuyển thành One Hot vector\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DXeo1j6cXU7",
        "outputId": "1aaa1a70-bbe1-4a58-ef88-3e4128ef25c4"
      },
      "source": [
        "label[i:i+1]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([351], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy7c6yxS0Lzu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "591580a5-c493-4901-ef14-377d30087298"
      },
      "source": [
        "for i in range(10):\n",
        "  print(\"{} ---> {}\".format(\" \".join(tokenizer.sequences_to_texts(predictors[i:i+1])), tokenizer.sequences_to_texts([label[i:i+1]])[0]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from ---> fairest\n",
            "from fairest ---> creatures\n",
            "from fairest creatures ---> we\n",
            "from fairest creatures we ---> desire\n",
            "from fairest creatures we desire ---> increase\n",
            "that ---> thereby\n",
            "that thereby ---> beauty's\n",
            "that thereby beauty's ---> rose\n",
            "that thereby beauty's rose ---> might\n",
            "that thereby beauty's rose might ---> never\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2z5du78YAoj"
      },
      "source": [
        "Chuyển từng nhãn thành vector one hot với số lượng là số từ trong từ điển"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWD-xDWzXT6y"
      },
      "source": [
        "label = ku.to_categorical(label, num_classes=total_words)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zscBhCtDkwK"
      },
      "source": [
        "## 4. Xây dựng model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Baru5NL1YMS_"
      },
      "source": [
        "Mạng bao gồm:\n",
        "\n",
        "- 1 lớp Embedding với chiều embedding là 100\n",
        "- Một lớp 1 Bidirectional với cell LSTM 150 node\n",
        "- Một lớp LSTM với 100 node\n",
        "- Mạng nơ ron phân loại gồm một lớp ẩn\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18r5pRWfEqup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c5e990a-9ea6-43c0-b86c-6f8a0235b7e1"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "\n",
        "model.add(Bidirectional(LSTM(150, return_sequences = True)))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(100))\n",
        "\n",
        "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 10, 100)           321100    \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 10, 300)          301200    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 10, 300)           0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               160400    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1605)              162105    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3211)              5156866   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,101,671\n",
            "Trainable params: 6,101,671\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKjWKga2YssS"
      },
      "source": [
        "Tiến hành training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bNWIxutDmd-"
      },
      "source": [
        " history = model.fit(predictors, label, epochs=130, verbose=0)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8heXdtslDpGU"
      },
      "source": [
        "### 5. Dự đoán 10 từ tiếp theo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io2XYlxpYztR"
      },
      "source": [
        "Câu mồi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNUO-xP4Y03Z"
      },
      "source": [
        "test_seq = 'despite of wrinkles'"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoMHQnCxY11s"
      },
      "source": [
        "Từ câu mồi\n",
        "- Dự đoán ra từ tiếp theo từ các từ của câu hiện tại\n",
        "- Nối từ đã được dự đoán vào câu hiện tại\n",
        "- Tiếp tục dùng câu hiện tại này để dự đoán\n",
        "- Thực hiện đến khi chiều dài của câu đạt một giới hạn cụ thể hoặc số từ sinh ra đạt giới hạn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnFicgjcDuqu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ea4a0cd-9811-4de8-acdb-4b2b064ac361"
      },
      "source": [
        "next_words = 10\n",
        "\n",
        "for _ in range(next_words):\n",
        "  # Chuyển câu thành vector\n",
        "  token_list = tokenizer.texts_to_sequences([test_seq])[0]\n",
        "  \n",
        "  # Padding câu\n",
        "  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "  \n",
        "  # Dự đoán từ tiếp theo\n",
        "  predicted = model.predict(token_list, verbose=0)\n",
        "  \n",
        "  output_word = \"\"\n",
        "\n",
        "  predicted_id = np.argmax(predicted)\n",
        "\n",
        "  if predicted_id in tokenizer.index_word:\n",
        "    output_word = tokenizer.index_word[predicted_id]\n",
        "    test_seq += \" \" + output_word\n",
        "  else:\n",
        "    break\n",
        "  \n",
        "print(test_seq)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "despite of wrinkles this thy golden time being hell of hide time being\n"
          ]
        }
      ]
    }
  ]
}